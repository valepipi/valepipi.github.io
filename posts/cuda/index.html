<!DOCTYPE html>
<html lang="zh-cn">
<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="color-scheme" content="light dark">


<meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com https://cdn.jsdelivr.net/; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;">




<meta name="author" content="游泳小狗">
<meta name="description" content="Cuda并行编程Cuda存储结构Global memory： 可读可写，最大最慢，所有Block共享，通常所说32G的GPU指的是global memory=32G Constant memory: 只读，所有Block共享 Texture memory: 只读，所有Block共享 GPU通过以上三种memory与Host上的memory交互 cudaMemcpy()
Shared memory: 可读可写，很快，较小，一个block内的Thread共享 Local memory: 可读可写，很快，很小，Thread专用 Registers: 可读可写，很快，很小，Thread专用 Cuda线程结构Grid:一组线程块(block) block：一组线程(thread) thread Cuda编程模型的关键术语host: CPU device: GPU host memory: 系统的内存 device memory：gpu卡上的存储器 kernels: GPU函数，由host发出，由device执行 device function: GPU函数，device执行，并且只能被device调用 线程索引如何定位到某一个线程？
计算该线程在所有线程中的索引
一维的线程索引 int index = threadIdx.x &#43; blockIdx.x * M = 5 &#43; 2 * 8 = 21 二维的线程索引 Thread_x = blockIdx.x * blockDim.x &#43;threadIdx.x = 6 Thread_y = blockIdx.">
<meta name="keywords" content="博客，个人">

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="cuda编程"/>
<meta name="twitter:description" content="Cuda并行编程Cuda存储结构Global memory： 可读可写，最大最慢，所有Block共享，通常所说32G的GPU指的是global memory=32G Constant memory: 只读，所有Block共享 Texture memory: 只读，所有Block共享 GPU通过以上三种memory与Host上的memory交互 cudaMemcpy()
Shared memory: 可读可写，很快，较小，一个block内的Thread共享 Local memory: 可读可写，很快，很小，Thread专用 Registers: 可读可写，很快，很小，Thread专用 Cuda线程结构Grid:一组线程块(block) block：一组线程(thread) thread Cuda编程模型的关键术语host: CPU device: GPU host memory: 系统的内存 device memory：gpu卡上的存储器 kernels: GPU函数，由host发出，由device执行 device function: GPU函数，device执行，并且只能被device调用 线程索引如何定位到某一个线程？
计算该线程在所有线程中的索引
一维的线程索引 int index = threadIdx.x &#43; blockIdx.x * M = 5 &#43; 2 * 8 = 21 二维的线程索引 Thread_x = blockIdx.x * blockDim.x &#43;threadIdx.x = 6 Thread_y = blockIdx."/>

<meta property="og:title" content="cuda编程" />
<meta property="og:description" content="Cuda并行编程Cuda存储结构Global memory： 可读可写，最大最慢，所有Block共享，通常所说32G的GPU指的是global memory=32G Constant memory: 只读，所有Block共享 Texture memory: 只读，所有Block共享 GPU通过以上三种memory与Host上的memory交互 cudaMemcpy()
Shared memory: 可读可写，很快，较小，一个block内的Thread共享 Local memory: 可读可写，很快，很小，Thread专用 Registers: 可读可写，很快，很小，Thread专用 Cuda线程结构Grid:一组线程块(block) block：一组线程(thread) thread Cuda编程模型的关键术语host: CPU device: GPU host memory: 系统的内存 device memory：gpu卡上的存储器 kernels: GPU函数，由host发出，由device执行 device function: GPU函数，device执行，并且只能被device调用 线程索引如何定位到某一个线程？
计算该线程在所有线程中的索引
一维的线程索引 int index = threadIdx.x &#43; blockIdx.x * M = 5 &#43; 2 * 8 = 21 二维的线程索引 Thread_x = blockIdx.x * blockDim.x &#43;threadIdx.x = 6 Thread_y = blockIdx." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://valepipi.github.io/posts/cuda/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-01-24T22:00:57+08:00" />
<meta property="article:modified_time" content="2023-01-24T22:00:57+08:00" /><meta property="og:site_name" content="游泳小狗" />




  <title>游泳小狗</title>

  
  <link rel="canonical" href="https://valepipi.github.io/posts/cuda/">
  

  <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>


  
  
  <link rel="stylesheet" href="/css/coder.min.6b1a4fbc48955b72aea7913e43fabeb45e8bc120da5aa41b598dd33adcac4b59.css" integrity="sha256-axpPvEiVW3Kup5E&#43;Q/q&#43;tF6LwSDaWqQbWY3TOtysS1k=" crossorigin="anonymous" media="screen" />





  
  
    
    
    <link rel="stylesheet" href="/css/coder-dark.min.39e41a7f16bdf8cb16e43cae7d714fa1016f1d2d2898a5b3f27f42c9979204e2.css" integrity="sha256-OeQafxa9&#43;MsW5DyufXFPoQFvHS0omKWz8n9CyZeSBOI=" crossorigin="anonymous" media="screen" />
  



   




  <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
<link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
<link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/images/safari-pinned-tab.svg" color="#5bbad5">


  

  <meta name="generator" content="Hugo 0.101.0" />


  

</head>







<body class="preload-transitions colorscheme-dark">
  
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


  <main class="wrapper">
    <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      游泳小狗
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/about/">关于我</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">博客</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/projects/">爱好</a>
            </li>
          
            <li class="navigation-item">
              <a class="navigation-link" href="/contact/">联系我</a>
            </li>
          
        
        
          
          
          
            
              
                <li class="navigation-item menu-separator">
                  <span>|</span>
                </li>
                
              
              <li class="navigation-item">
                <a href="https://valepipi.github.io/en/posts/cuda/">英文</a>
              </li>
            
          
        
      </ul>
    
  </section>
</nav>


    <div class="content">
      
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://valepipi.github.io/posts/cuda/">
              cuda编程
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime="2023-01-24T22:00:57&#43;08:00">
                January 24, 2023
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              阅读时间：2 分钟
            </span>
          </div>
          
          <div class="categories">
  <i class="fa fa-folder" aria-hidden="true"></i>
    <a href="/categories/blog/">blog</a></div>

          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/tech/">tech</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <h1 id="cuda并行编程">
  Cuda并行编程
  <a class="heading-link" href="#cuda%e5%b9%b6%e8%a1%8c%e7%bc%96%e7%a8%8b">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h1>
<hr>
<h2 id="cuda存储结构">
  Cuda存储结构
  <a class="heading-link" href="#cuda%e5%ad%98%e5%82%a8%e7%bb%93%e6%9e%84">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p><img src="/images/cuda-memory.png" alt="cuda-memory"></p>
<ul>
<li>Global memory： 可读可写，最大最慢，所有Block共享，通常所说<em>32G的GPU</em>指的是global memory=32G</li>
<li>Constant memory: 只读，所有Block共享</li>
<li>Texture memory: 只读，所有Block共享</li>
</ul>
<blockquote>
<p>GPU通过以上三种memory与Host上的memory交互  <strong>cudaMemcpy()</strong></p>
</blockquote>
<ul>
<li>Shared memory: 可读可写，很快，较小，一个block内的Thread共享</li>
<li>Local memory: 可读可写，很快，很小，Thread专用</li>
<li>Registers: 可读可写，很快，很小，Thread专用</li>
</ul>
<hr>
<h2 id="cuda线程结构">
  Cuda线程结构
  <a class="heading-link" href="#cuda%e7%ba%bf%e7%a8%8b%e7%bb%93%e6%9e%84">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p><img src="/images/cuda-thread.png" alt="cuda-thread"></p>
<ul>
<li>Grid:一组线程块(block)</li>
<li>block：一组线程(thread)</li>
<li>thread</li>
</ul>
<hr>
<h2 id="cuda编程模型的关键术语">
  Cuda编程模型的关键术语
  <a class="heading-link" href="#cuda%e7%bc%96%e7%a8%8b%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%85%b3%e9%94%ae%e6%9c%af%e8%af%ad">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<ul>
<li>host: CPU</li>
<li>device: GPU</li>
<li>host memory: 系统的内存</li>
<li>device memory：gpu卡上的存储器</li>
<li>kernels: GPU函数，由host发出，由device执行</li>
<li>device function: GPU函数，device执行，并且只能被device调用</li>
</ul>
<hr>
<h2 id="线程索引">
  线程索引
  <a class="heading-link" href="#%e7%ba%bf%e7%a8%8b%e7%b4%a2%e5%bc%95">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p><strong>如何定位到某一个线程？</strong></p>
<blockquote>
<blockquote>
<p>计算该线程在所有线程中的索引</p>
</blockquote>
</blockquote>
<ul>
<li>一维的线程索引</li>
</ul>
<p><img src="/images/cuda-thread-index.png" alt="cuda-thread-index"></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-C++" data-lang="C++"><span style="display:flex;"><span><span style="">int</span> index = threadIdx.x + blockIdx.x * M
</span></span><span style="display:flex;"><span>    = 5 + 2 * 8
</span></span><span style="display:flex;"><span>    = 21
</span></span></code></pre></div><ul>
<li>二维的线程索引</li>
</ul>
<p><img src="/images/cuda-thread-index2.png" alt="cuda-thread-index2"></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-C++" data-lang="C++"><span style="display:flex;"><span>Thread_x = blockIdx.x * blockDim.x +threadIdx.x = 6
</span></span><span style="display:flex;"><span>Thread_y = blockIdx.y * blockDim.y +threadIdx.y = 3
</span></span></code></pre></div><p><strong>如何给矩阵运算安排线程？</strong></p>
<p>当把整个Grid对应到一个矩阵，则该<strong>线程的索引</strong>正好对应到<strong>矩阵的坐标</strong>即可，意为Grid中的一个线程处理矩阵的一个元素。</p>
<p><strong>如果矩阵的规模大于线程总数？</strong></p>
<ul>
<li>grid-sride-loop：每个Grid中的线程循环使用，负责计算绿色框内的子矩阵；其中的每个block中的线程循环使用，负责计算对应颜色的子矩阵</li>
</ul>
<p><img src="/images/grid-stride-loop.png" alt="grid-stride-loop"></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-C++" data-lang="C++"><span style="display:flex;"><span>tx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
</span></span><span style="display:flex;"><span>ty = cuda.blockIdx.x * cuda.blockDim.y + cuda.threadIdx.y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>stride_x = blockDim.x * gridDim.x
</span></span><span style="display:flex;"><span>stride_y = blockDim.y * gridDim.y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">for</span> (<span style="">int</span> y = ty; y &lt; height; y+=stride_y){
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span>(<span style="">int</span> x = tx; x &lt; width; x+=stride_x){
</span></span><span style="display:flex;"><span>        matrix[x,y] = ...
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><hr>
<h2 id="cuda矩阵计算">
  CUDA矩阵计算
  <a class="heading-link" href="#cuda%e7%9f%a9%e9%98%b5%e8%ae%a1%e7%ae%97">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<!-- raw HTML omitted -->
<p><img src="/images/conv-to-mm.png" alt="conv-to-mm"></p>
<p>在cuda中，将逐步的卷积计算展开为两个大矩阵相乘，用空间换时间。</p>
<ol>
<li>一个颜色表示一个通道</li>
<li>图中的卷积核经过了转置</li>
</ol>
<!-- raw HTML omitted -->
<p><img src="/images/matrix-multi.png" alt="matrix-multi"></p>
<p>一个线程负责得到结果中的<strong>一个元素</strong>：</p>
<ol>
<li>读取矩阵A对应的一行(从global memory)</li>
<li>读取矩阵B对应的一列(从global memory)</li>
<li>该行该列的元素相乘再相加，得到结果(一个元素)</li>
</ol>
<p>所有线程并行计算，得到结果中的所有元素。</p>
<p>优化：如果矩阵规模过大，一行or一列读取的时间过长，<strong>利用shared memory每次读取A,B的一个子矩阵，再截取其中的一小截数据读入，分次计算，累加</strong> (shared memory小而快)</p>
<p><img src="/images/matrix_multi-sharedmemory.png" alt="matrix_multi-sharedmemory"></p>
<h4 id="代码">
  代码
  <a class="heading-link" href="#%e4%bb%a3%e7%a0%81">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h4>
<ul>
<li>cpu实现矩阵乘法</li>
<li>gpu实现矩阵乘法，仅从global memory读取</li>
<li>gpu实现矩阵乘法，分块，从shared memory读取</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>TPB = 16
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>@numba.jit(nopython=<span style="font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span><span style="font-weight:bold">def</span> matmul_cpu(A, B, C):
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span> y <span style="font-weight:bold">in</span> range(B.shape[1]):
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> x <span style="font-weight:bold">in</span> range(A.shape[0]):
</span></span><span style="display:flex;"><span>            tmp = 0.
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> k <span style="font-weight:bold">in</span> range(A.shape[1]):
</span></span><span style="display:flex;"><span>            tmp += A[x,k]*B[k,y]
</span></span><span style="display:flex;"><span>        C[x,y] = tmp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>@cuda.jit
</span></span><span style="display:flex;"><span><span style="font-weight:bold">def</span> matmul_gpu(A, B, C):
</span></span><span style="display:flex;"><span>    row, col = cuda.grid(2)
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">if</span> row &lt; C.shape[0] <span style="font-weight:bold">and</span> col &lt; C.shape[1]: <span style="font-style:italic"># 每行每列并行计算</span>
</span></span><span style="display:flex;"><span>        tmp = 0.
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> k <span style="font-weight:bold">in</span> range(A.shape[1]):
</span></span><span style="display:flex;"><span>            tmp += A[row, k]*B[k, col]
</span></span><span style="display:flex;"><span>        C[row, col] = tmp
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>@cuda.jit
</span></span><span style="display:flex;"><span><span style="font-weight:bold">def</span> matmul_shared_mem(A, B, C):
</span></span><span style="display:flex;"><span>    sA = cuda.shared.array(shape=(TPB,TPB), dtype=float32) <span style="font-style:italic">#矩阵A的子矩阵</span>
</span></span><span style="display:flex;"><span>    sB = cuda.shared.array(shape=(TPB,TPB), dtype=float32) <span style="font-style:italic">#矩阵B的子矩阵</span>
</span></span><span style="display:flex;"><span>    x, y = cuda.grid(2)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    tx = cuda.threadIdx.x
</span></span><span style="display:flex;"><span>    ty = cuda.threadIdex.y
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">if</span> x &gt;= C.shape[0] <span style="font-weight:bold">and</span> y &gt;= C.shape[1]:
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    tmp = 0.
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span> i <span style="font-weight:bold">in</span> range(int(A.shape[1])/TPB):
</span></span><span style="display:flex;"><span>        sA[tx, ty] = A[x, ty + i*TPB] <span style="font-style:italic"># 一次计算中的那一列</span>
</span></span><span style="display:flex;"><span>        sB[tx, ty] = B[tx + i*TPB, y] <span style="font-style:italic"># 一次计算中的那一行</span>
</span></span><span style="display:flex;"><span>        cuda.syncthreads()
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">for</span> j <span style="font-weight:bold">in</span> range(TPB):
</span></span><span style="display:flex;"><span>            tmp += sA[tx, j]*sB[j , ty]
</span></span><span style="display:flex;"><span>        cuda.syncthreads()
</span></span><span style="display:flex;"><span>    C[x,y] = tmp
</span></span></code></pre></div><p>测试函数</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>A = numpy.full((TPB*50, TPB*50), 3, numpy.float)
</span></span><span style="display:flex;"><span>B = numpy.full((TPB*50, TPB*50), 4, numpy.float)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>C_cpu = numpy.full((A.shape[0], B.shape[1]), 0, numpy.float)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="font-style:italic">&#34;Start processing in CPU&#34;</span>)
</span></span><span style="display:flex;"><span>start_cpu = time.time()
</span></span><span style="display:flex;"><span>matmul_cpu(A,B,C_cpu)
</span></span><span style="display:flex;"><span>end_cpu = time.time()
</span></span><span style="display:flex;"><span>time_cpu = (end_cpu - start_cpu)
</span></span><span style="display:flex;"><span>print(<span style="font-style:italic">&#34;CPU Time&#34;</span>+ str(time_cpu))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-style:italic"># Start in GPU</span>
</span></span><span style="display:flex;"><span>A_global_mem = cuda.to_device(A)
</span></span><span style="display:flex;"><span>B_global_mem = cuda.to_device(B)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>C_global_mem = cuda.device_array((A.shape[0], B.shape[1]))
</span></span><span style="display:flex;"><span>C_shared_mem = cuda.device_array((A.shape[0], B.shape[1]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>threadsperblock = (TPB, TPB)
</span></span><span style="display:flex;"><span>blockspergrid_x = int(math.ceil(A.shape[0]/threadsperblock[0]))
</span></span><span style="display:flex;"><span>blockspergrid_y = int(math.ceil(B.shape[1]/threadsperblock[1]))
</span></span><span style="display:flex;"><span>blockspergrid = (blockspergrid_x, blockspergrid_y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="font-style:italic">&#34;Start processing in GPU&#34;</span>)
</span></span><span style="display:flex;"><span>start_gpu = time.time()
</span></span><span style="display:flex;"><span>matmul_gpu[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)
</span></span><span style="display:flex;"><span>cuda.synchronize()
</span></span><span style="display:flex;"><span>end_gpu = time.time()
</span></span><span style="display:flex;"><span>time_gpu = (end_gpu - start_gpu)
</span></span><span style="display:flex;"><span>print(<span style="font-style:italic">&#34;GPU Time (global memory)&#34;</span> + str(time_gpu))
</span></span><span style="display:flex;"><span>C_global_gpu = C_global_mem.copy_to_host()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>start_gpu_shared = time.time()
</span></span><span style="display:flex;"><span>matmul_shared_mem[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_shared_mem)
</span></span><span style="display:flex;"><span>cuda.synchronize()
</span></span><span style="display:flex;"><span>end_gpu_shared = time.time()
</span></span><span style="display:flex;"><span>time_gpu_shared = (end_gpu_shared -start_gpu_shared)
</span></span><span style="display:flex;"><span>print(<span style="font-style:italic">&#34;GPU Time (shared memory)&#34;</span> + str(time_gpu_shared))
</span></span><span style="display:flex;"><span>C_shared_gpu = C_shared_mem.copy_to_host()
</span></span></code></pre></div>
      </div>


      <footer>
        


        
        <div id="commento"></div>
<script src="https://cdn.commento.io/js/commento.js"></script>
        
      </footer>
    </article>

    
  </section>

    </div>

    <footer class="footer">
  <section class="container">
    ©
    
      2022 -
    
    2023
     游泳小狗 
    ·
    
    技术支持 <a href="https://gohugo.io/">Hugo</a> & <a href="https://github.com/luizdepra/hugo-coder/">Coder</a>.
    
  </section>
</footer>

  </main>

  
  
  <script src="/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js" integrity="sha256-I2BJOV3DaC&#43;ycZZAhylY4S8fJAZ7sJwyeyM&#43;YpDH7aw="></script>
  

  

  

  <script>
(function(f, a, t, h, o, m){
	a[h]=a[h]||function(){
		(a[h].q=a[h].q||[]).push(arguments)
	};
	o=f.createElement('script'),
	m=f.getElementsByTagName('script')[0];
	o.async=1; o.src=t; o.id='fathom-script';
	m.parentNode.insertBefore(o,m)
})(document, window, '//cdn.usefathom.com/tracker.js', 'fathom');
fathom('set', 'siteId', 'ABCDE');
fathom('trackPageview');
</script>


  <script async defer data-domain="example.com" src="https://plausible.io/js/plausible.js"></script>


  <script data-goatcounter="https://code.goatcounter.com/count"
        async src="//gc.zgo.at/count.js"></script>


  
<script defer src='https://static.cloudflareinsights.com/beacon.min.js'
        data-cf-beacon='{"token": "token"}'></script>



  <script type="application/javascript">
  var _paq = window._paq = window._paq || [];
   
  _paq.push(['trackPageView']);
  _paq.push(['enableLinkTracking']);
  (function() {
    var u="https://analytics.example.com/";
    _paq.push(['setTrackerUrl', u+'matomo.php']);
    _paq.push(['setSiteId', 'ABCDE']);
    var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
    g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
  })();
</script>


  
<script async src="https://www.googletagmanager.com/gtag/js?id=gid"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'gid');
</script>


  <script type="text/javascript">
    !function(T,l,y){var S=T.location,k="script",D="connectionString",C="ingestionendpoint",I="disableExceptionTracking",E="ai.device.",b="toLowerCase",w="crossOrigin",N="POST",e="appInsightsSDK",t=y.name||"appInsights";(y.name||T[e])&&(T[e]=t);var n=T[t]||function(d){var g=!1,f=!1,m={initialize:!0,queue:[],sv:"5",version:2,config:d};function v(e,t){var n={},a="Browser";return n[E+"id"]=a[b](),n[E+"type"]=a,n["ai.operation.name"]=S&&S.pathname||"_unknown_",n["ai.internal.sdkVersion"]="javascript:snippet_"+(m.sv||m.version),{time:function(){var e=new Date;function t(e){var t=""+e;return 1===t.length&&(t="0"+t),t}return e.getUTCFullYear()+"-"+t(1+e.getUTCMonth())+"-"+t(e.getUTCDate())+"T"+t(e.getUTCHours())+":"+t(e.getUTCMinutes())+":"+t(e.getUTCSeconds())+"."+((e.getUTCMilliseconds()/1e3).toFixed(3)+"").slice(2,5)+"Z"}(),name:"Microsoft.ApplicationInsights."+e.replace(/-/g,"")+"."+t,sampleRate:100,tags:n,data:{baseData:{ver:2}}}}var h=d.url||y.src;if(h){function a(e){var t,n,a,i,r,o,s,c,u,p,l;g=!0,m.queue=[],f||(f=!0,t=h,s=function(){var e={},t=d.connectionString;if(t)for(var n=t.split(";"),a=0;a<n.length;a++){var i=n[a].split("=");2===i.length&&(e[i[0][b]()]=i[1])}if(!e[C]){var r=e.endpointsuffix,o=r?e.location:null;e[C]="https://"+(o?o+".":"")+"dc."+(r||"services.visualstudio.com")}return e}(),c=s[D]||d[D]||"",u=s[C],p=u?u+"/v2/track":d.endpointUrl,(l=[]).push((n="SDK LOAD Failure: Failed to load Application Insights SDK script (See stack for details)",a=t,i=p,(o=(r=v(c,"Exception")).data).baseType="ExceptionData",o.baseData.exceptions=[{typeName:"SDKLoadFailed",message:n.replace(/\./g,"-"),hasFullStack:!1,stack:n+"\nSnippet failed to load ["+a+"] -- Telemetry is disabled\nHelp Link: https://go.microsoft.com/fwlink/?linkid=2128109\nHost: "+(S&&S.pathname||"_unknown_")+"\nEndpoint: "+i,parsedStack:[]}],r)),l.push(function(e,t,n,a){var i=v(c,"Message"),r=i.data;r.baseType="MessageData";var o=r.baseData;return o.message='AI (Internal): 99 message:"'+("SDK LOAD Failure: Failed to load Application Insights SDK script (See stack for details) ("+n+")").replace(/\"/g,"")+'"',o.properties={endpoint:a},i}(0,0,t,p)),function(e,t){if(JSON){var n=T.fetch;if(n&&!y.useXhr)n(t,{method:N,body:JSON.stringify(e),mode:"cors"});else if(XMLHttpRequest){var a=new XMLHttpRequest;a.open(N,t),a.setRequestHeader("Content-type","application/json"),a.send(JSON.stringify(e))}}}(l,p))}function i(e,t){f||setTimeout(function(){!t&&m.core||a()},500)}var e=function(){var n=l.createElement(k);n.src=h;var e=y[w];return!e&&""!==e||"undefined"==n[w]||(n[w]=e),n.onload=i,n.onerror=a,n.onreadystatechange=function(e,t){"loaded"!==n.readyState&&"complete"!==n.readyState||i(0,t)},n}();y.ld<0?l.getElementsByTagName("head")[0].appendChild(e):setTimeout(function(){l.getElementsByTagName(k)[0].parentNode.appendChild(e)},y.ld||0)}try{m.cookie=l.cookie}catch(p){}function t(e){for(;e.length;)!function(t){m[t]=function(){var e=arguments;g||m.queue.push(function(){m[t].apply(m,e)})}}(e.pop())}var n="track",r="TrackPage",o="TrackEvent";t([n+"Event",n+"PageView",n+"Exception",n+"Trace",n+"DependencyData",n+"Metric",n+"PageViewPerformance","start"+r,"stop"+r,"start"+o,"stop"+o,"addTelemetryInitializer","setAuthenticatedUserContext","clearAuthenticatedUserContext","flush"]),m.SeverityLevel={Verbose:0,Information:1,Warning:2,Error:3,Critical:4};var s=(d.extensionConfig||{}).ApplicationInsightsAnalytics||{};if(!0!==d[I]&&!0!==s[I]){var c="onerror";t(["_"+c]);var u=T[c];T[c]=function(e,t,n,a,i){var r=u&&u(e,t,n,a,i);return!0!==r&&m["_"+c]({message:e,url:t,lineNumber:n,columnNumber:a,error:i}),r},d.autoExceptionInstrumented=!0}return m}(y.cfg);function a(){y.onInit&&y.onInit(n)}(T[t]=n).queue&&0===n.queue.length?(n.queue.push(a),n.trackPageView({})):a()}(window,document,{
    src: "https://js.monitor.azure.com/scripts/b/ai.2.min.js", 
    
    
    
    crossOrigin: "anonymous", 
    
    cfg: { 
        connectionString: "connectionstring"
         
    }});
    </script>    

</body>

</html>
