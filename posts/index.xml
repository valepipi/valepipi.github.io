<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 游泳小狗</title>
    <link>https://valepipi.github.io/posts/</link>
    <description>Recent content in Posts on 游泳小狗</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Mon, 30 Jan 2023 16:06:16 +0800</lastBuildDate><atom:link href="https://valepipi.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ReRAM存算一体加速CNN</title>
      <link>https://valepipi.github.io/posts/reram/</link>
      <pubDate>Mon, 30 Jan 2023 16:06:16 +0800</pubDate>
      
      <guid>https://valepipi.github.io/posts/reram/</guid>
      <description>ReRAM器件特性ReRAM 是一种无源双端型阻性器件，可储存多个阻态(MLC)。
存储模式VS计算模式：一般的，ReRAM 阵列在进行计算操作时需要将所有的输入端口打开。
优势可作为存储元件，将数据以电阻/电导的形式储存在器件中；
可以依据欧姆定律作为模拟计算元件，实现电导与电压相乘的运算。而 ReRAM交叉阵列结构使得同一列的输出电流能够直接在位线上汇聚，与乘累加运算相匹配，凭借这种原理 ReRAM 阵列可以直接在存储本地实现矩阵-向量乘法运算，突破了传统 CPU 存算分离的架构。
缺点多值存储不稳定：目前 ReRAM 器件工艺还不成熟，多阻态之间的阈值区间间隔不大，导致 ReRAM 作为多阻态器件不稳定，目前已有 的基于 ReRAM 的神经网络研究都是将 ReRAM 当作**单比特器件(SLC)**来使用
写入比读取开销大得多：不同于传统存储器件，ReRAM 的阻 变机制导致其读写操作的延时和能耗严重不均衡，对于 ReRAM 多值器件来说，写 操作所需的时间长、输入电压大，写操作的延时和能耗是读操作的 10 倍以上。
ReRAM加速器与其他模块的关系：
ReRAM加速神经网络的关键问题不能直接支持超大规模卷积神经网络(如，三维神经网络)的高能效运算： 相比于二维卷积神经网络，三维卷积神经网络需要的参数量和训练所需的内存容量呈指数倍增长，而将三维卷积神经网络映射到二维 ReRAM 阵列会使阵列面积大幅增加，从而导致串扰等现象更加严重，影响算法的精度。 包含以下两个关键问题： 如何将 卷积核运算转换为适合 ReRAM 计算结构处理的矩阵-向量乘法运算，并使得该转 换引入尽可能少的额外数据传输代价 如何高能效的将多比特神经网络权重值存储到单比特 ReRAM 器件 CNN中运算的大量数据迁移CNN末端的全连接层：需要读取大量的权重矩阵，并与该层的输入向量进行矩阵-向量乘法运算。(得到的输出向量既可直接作为识别结果输出， 又可以通过非线性神经元函数作为下一个全连接层的输入向量。)
CNN中的卷积核运算(占比95%以上)：由于卷积核的每一个元素都是训练得到的神经网络参数，因此卷积核运算中需要反复从存储数据中读取卷积核参数，并与输入特征图进行卷积运算。
CNN卷积核运算映射到二维ReRAM卷积核运算：将两个三维矩阵中的对应元素进行乘加运算；如果将两个三维矩阵按照相同顺序进行完全地展开，那么卷积运算就可以看作为 向量的内积运算。
基于矩阵-向量乘法运算的原理，有两种可能实现的卷积运算映射方法：
特征图数据作为电导值写入到 ReRAM 阵列单元中，卷积核作为 ReRAM 阵列端口的输入电压向量：
可将相同卷积核对应的不同滑窗位置的特征图数据映射到多个 ReRAM 列中，这样可以在同一时钟周期下计算出不同滑窗位置上的卷积结果，而在不同周期下能够计算出多个卷积核的卷积结果。
优点：对于单个特征图的卷积运算有较大的并行度
缺点：(1)进行不同特征图的运算时需要对ReRAM 阵列进行反复擦写，并且写开销远大于读开销; (2)使得神经网络的后一层运算需要等上一层的运算结果写入到 ReRAM 阵列中才能进行运算。</description>
    </item>
    
    <item>
      <title>神经网络基础</title>
      <link>https://valepipi.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Mon, 30 Jan 2023 14:45:42 +0800</pubDate>
      
      <guid>https://valepipi.github.io/posts/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</guid>
      <description>神经网络基本概念神经网络分为两个阶段的运算：训练、推理
训练——过程视角正向传播：目的是使用当前未训练好的网络对训练样本进行推理；过程和推理过程完全一致，包含大量的矩阵-向量乘法运算 反向传播：目的是根据正向传播获得的计算误差，依次计算并存储各层的中间变量以及参数的梯度，包含大量反向的矩阵-向量乘法运算 权重更新：在正向或反向传播的过程中都会进行权重更新，需要计算各权重矩阵的梯度大小，得到权重的更新值，实现模型的更新 推理——结构视角全连接层
卷积层
目的：利用卷积运算的局部特性提取图像的局部特征并逐层抽象 与全连接层的区别：卷积层可以用较少的模型参数实 现对大规模图像的特征提取 特征图(feature map)：在卷积层间传输的特征数据；输入特征图——&amp;gt;卷积核——&amp;gt;输出特征图 卷积核(Convolution Kernel)：卷积核运算是卷积层中的主要运算，涉及大量的数据计算和数据传输。卷积核运算是将两个三维矩阵中的对应元素进行乘法运算，再将乘得的结果进行累加计算。 S为卷积核的行/列的数量，I 为输入特征图的通道数，即一个卷积核的大小为 S×S×I；
假设通道数=1时，一个卷积核在特征图上扫过的面积就是输出结果的size：一个卷积核扫过特征图上的一个区域就得到一个值(size=1*1)，把所有区域的值按照行列拼接起来就是结果(size=扫过的面积)其他：级联在两层卷积运算中间的辅助运算，如非线性神经元函数(ReLU/Sigmoid)、池化(Pooling)等 非线性神经元函数
目的：提供非线性分类能力，广泛用于传统全连接神经网络、卷积神经网络等各类神经网络算法中 在CNN中，通常级联在卷积核运算后 池化函数
目的：对特征图进行空间压缩,降低特征图的维度 做法：把特征图相邻范围内的所有元素 用一个值替代，从而在保持局部特征的同时，降低卷积神经网络的计算量和参数量 与卷积核运算的区别：池化函数尽管使用与卷积核相同的滑窗形式，在全特征图中进行运算，但是*步长（Stride）*运算形式会令同一个特征图元素不会被包含至多个池化区域内。 形式：平均池化（Mean Pooling）、最大池化（Max Pooling） 相比于平均池化，最大池化更容易保留特征图中最鲜明的特征信息</description>
    </item>
    
    <item>
      <title>cuda编程</title>
      <link>https://valepipi.github.io/posts/cuda/</link>
      <pubDate>Tue, 24 Jan 2023 22:00:57 +0800</pubDate>
      
      <guid>https://valepipi.github.io/posts/cuda/</guid>
      <description>Cuda并行编程Cuda存储结构Global memory： 可读可写，最大最慢，所有Block共享，通常所说32G的GPU指的是global memory=32G Constant memory: 只读，所有Block共享 Texture memory: 只读，所有Block共享 GPU通过以上三种memory与Host上的memory交互 cudaMemcpy()
Shared memory: 可读可写，很快，较小，一个block内的Thread共享 Local memory: 可读可写，很快，很小，Thread专用 Registers: 可读可写，很快，很小，Thread专用 Cuda线程结构Grid:一组线程块(block) block：一组线程(thread) thread Cuda编程模型的关键术语host: CPU device: GPU host memory: 系统的内存 device memory：gpu卡上的存储器 kernels: GPU函数，由host发出，由device执行 device function: GPU函数，device执行，并且只能被device调用 </description>
    </item>
    
    <item>
      <title>🐰LeetCode Hot100 题解</title>
      <link>https://valepipi.github.io/posts/leetcode/</link>
      <pubDate>Sat, 21 Jan 2023 19:57:59 +0800</pubDate>
      
      <guid>https://valepipi.github.io/posts/leetcode/</guid>
      <description>😃数学问题1. 两数之和给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。
你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 你可以按任意顺序返回答案。
输入：nums = [2,7,11,15], target = 9 输出：[0,1] 解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 class Solution: def twoSum(self, nums: List[int], target: int) -&amp;gt; List[int]: &amp;#34;&amp;#34;&amp;#34; 1. 暴力查找O(n^2) 2. 利用dict反查O(n) &amp;#34;&amp;#34;&amp;#34; # n = len(nums) # for i in range(n): # for j in range(i + 1, n): # if nums[i] + nums[j] == target: # return [i ,j] # return [] dict = {} for i in range( len(nums) ): if target - nums[i] not in dict: dict[nums[i]] = i #建立反查表 else: return [dict[target-nums[i]], i] 88.</description>
    </item>
    
    <item>
      <title>SQL题目</title>
      <link>https://valepipi.github.io/posts/sql%E9%A2%98%E7%9B%AE/</link>
      <pubDate>Mon, 27 Jun 2022 18:57:30 +0800</pubDate>
      
      <guid>https://valepipi.github.io/posts/sql%E9%A2%98%E7%9B%AE/</guid>
      <description>题目1：⛺查询并计算留存率留存率：留存用户数/总用户数 留存用户：第一天登录，且第二天也登录的用户 四舍五入保留3位小数：round(原始数字，3) 根据得到后一天的日期：DATE_ADD(当天日期yy-mm-dd, INTERVAL 1 DAY) 登录的第一天： min(date) Select round(count(distinct user_id)*1.0/(select count(distinct user_id) from login) ,3) from login where (user_id,date) in (select user_id, DATE_ADD(min(date),INTERVAL 1 DAY) from login group by user_id) </description>
    </item>
    
    <item>
      <title>搭建个人博客</title>
      <link>https://valepipi.github.io/posts/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</link>
      <pubDate>Mon, 27 Jun 2022 18:21:48 +0800</pubDate>
      
      <guid>https://valepipi.github.io/posts/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</guid>
      <description>Hugo+Github搭建博客 参考链接：
Github Pages + Hugo 搭建个人博客
配置主题：
Hugo-Coder主题配置
美化：
插入emoji
更新博客： 先 cd 到 myblog/content 目录下，然后写 md 文章 写好以后，cd 到myblog 下执行 hugo 命令重新生成 public 文件 再cd 进 myblog/public 中，然后执行 git 提交命令提交到 github 即可 # 1.新建一篇文章，在网站根目录：H:\HugoWebsite\blog hugo new posts/测试博客.md # 2.在目录 H:\HugoWebsite\blog\content\posts 下找到对应文件进行修改 # 3.本地预览, http://localhost:1313/ hugo server -D # 4.构建 Hugo 网站 hugo # 5.切换到目录 H:\HugoWebsite\blog\public ，提交修改至本地库 git add . git commit -m &amp;#39;commit info&amp;#39; # 6.将修改推至远程库 git push -u origin master </description>
    </item>
    
  </channel>
</rss>
